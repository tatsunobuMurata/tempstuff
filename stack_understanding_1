Run these searches under monitoring console app if it is connected to read your indexer data. If not, then run them from search heads.

<001 - IDX tput > <share a screenshot of a line graph>

group=per_Index_thruput index=_internal source="*metrics.log" sourcetype=splunkd earliest=-w@w+1d latest=-w@w+2d
| eval ingest_pipe=if(isnotnull(ingest_pipe),ingest_pipe,"none")
| search ingest_pipe=*
| eval mb=kb/1024
| timechart minspan=5m per_second(mb) as MBps


<002 - license> <share a screenshot of the table>
index=_internal source=*license* TERM(RolloverSummary) earliest=-8d@d latest=-0d@d 
| bin _time span=1d 
| stats latest(b) AS b by slave, pool, _time 
| timechart span=1d sum(b) AS "volume" fixedrange=true 
| eval ingestGB_rollover=round((((volume / 1024) / 1024) / 1024),3) 
| fields - volume | eval _time = _time+86399

<003 - compute-1 - filter it with host=indexers only> <share a screenshot of a line graph>

index=_introspection host=idx* sourcetype=splunk_resource_usage component=Hostwide earliest=-w@w+1d latest=-w@w+2d
| eval cpu_busy='data.cpu_system_pct'+'data.cpu_user_pct' 
| timechart span=5m limit=100 useother=false p90(cpu_busy) as cpu_busy by host

<003-1 - compute-1 - filter it with host=indexers only> <share a screenshot of a line graph>

index=_introspection host=idx* sourcetype=splunk_resource_usage component=Hostwide earliest=-w@w+1d latest=-w@w+2d
| eval cpu_busy='data.cpu_system_pct'+'data.cpu_user_pct' 
| timechart span=5m limit=100 useother=false p99(cpu_busy) as cpu_busy by host



<004 - compute-2- filter it with host=indexers only> <share a screenshot of a line graph>

index=_introspection host=idx* sourcetype=splunk_resource_usage component=Hostwide earliest=-w@w+1d latest=-w@w+2d
| eval cpu_busy='data.cpu_system_pct'+'data.cpu_user_pct' 
| timechart span=5m limit=100 useother=false p90(data.normalized_load_avg_1min) as load_avg by host


<004 -1 - server info>

| rest timeout=600 splunk_server=*idx* /servicesNS/-/-/server/info 
| fields splunk_server, numberOfVirtualCores, numberOfCores 
| eval cpu_core_count = if(isnotnull(numberOfVirtualCores), numberOfVirtualCores, numberOfCores) 
| eval potentialHTOffAlarm = if(isnotnull(numberOfVirtualCores), "yes", "no")


<005 - search-1> <share a screenshot of the table output, if too big then share the csv export>

index=_audit TERM(action=search) sourcetype=audittrail search_id!="'rsa_*" earliest=-w@w+1d latest=-w@w+2d 
| eval user = if(user="n/a", null(), user) 
| rex field=_raw "[^\_]index=\"?(?<Index>[\_a-zA-Z\-\:]{2,})\"?" max_match=0 
| eval Index=lower(Index) 
| rex field=search_id "\'(?P<search_id>.*?)\'" 
| eval search=if(isnull(savedsearch_name) OR savedsearch_name=="", search, savedsearch_name) 
| eval search_type=case(match(search_id,"^SummaryDirector_"),"summarization",match(savedsearch_name,"^_ACCELERATE_"),"acceleration",match(search_id,"^((rt_)?scheduler_|alertsmanager_)"),"scheduled",match(search_id,"\\d{10}\\.\\d+(_[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12})?$"),"ad hoc",true(),"other") 
| stats min(_time) as _time, values(user) as user, latest(info) as status, max(total_run_time) as total_run_time, values(Index) as Index , first(search) as search, first(search_type) as search_type, values(app) as app, first(apiStartTime) as apiStartTime, first(apiEndTime) as apiEndTime , first(search_et) as search_et, first(search_lt) as search_lt, max(searched_buckets) as searched_buckets, max(scan_count) as scan_count, max(considered_events) as considered_events, max(event_count) as event_count, max(result_count) as result_count, first(exec_time) as exec_time by search_id
| stats count, p90(total_run_time) by search_type status
| eventstats sum(count) as total_searches

<006 - search-2> <share a screenshot of the table output, if too big then share the csv export>

index=_audit TERM(action=search) sourcetype=audittrail search_id!="'rsa_*" earliest=-w@w+1d latest=-w@w+2d 
| eval user = if(user="n/a", null(), user) 
| rex field=_raw "[^\_]index=\"?(?<Index>[\_a-zA-Z\-\:]{2,})\"?" max_match=0 
| eval Index=lower(Index) 
| rex field=search_id "\'(?P<search_id>.*?)\'" 
| eval search=if(isnull(savedsearch_name) OR savedsearch_name=="", search, savedsearch_name) 
| eval search_type=case(match(search_id,"^SummaryDirector_"),"summarization",match(savedsearch_name,"^_ACCELERATE_"),"acceleration",match(search_id,"^((rt_)?scheduler_|alertsmanager_)"),"scheduled",match(search_id,"\\d{10}\\.\\d+(_[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12})?$"),"ad hoc",true(),"other") 
| stats min(_time) as _time, values(user) as user, latest(info) as status, max(total_run_time) as total_run_time, values(Index) as Index , first(search) as search, first(search_type) as search_type, values(app) as app, first(apiStartTime) as apiStartTime, first(apiEndTime) as apiEndTime , first(search_et) as search_et, first(search_lt) as search_lt, max(searched_buckets) as searched_buckets, max(scan_count) as scan_count, max(considered_events) as considered_events, max(event_count) as event_count, max(result_count) as result_count, first(exec_time) as exec_time by search_id
| stats count, p90(total_run_time) by search_type
| eventstats sum(count) as total_searches


<007 - search-3> <share a screenshot of the table output, if too big then share the csv export>

( index=_internal sourcetype=scheduler  (status="completed" OR status="deferred" OR status="skipped")) earliest=-w@w+1d latest=-w@w+2d
| rex "alert_actions=\"(?P<alert_actions>.*?)\","
| eventstats count(eval(status=="completed" OR status=="skipped")) AS total_exec_nosplit, count(eval(status=="skipped")) AS skipped_exec_nosplit
| stats values(total_exec_nosplit) as total_exec_nosplit, values(skipped_exec_nosplit) as skipped_exec_nosplit, values(alert_actions) as alert_actions, values(search_type) as search_type count(eval(status=="completed" OR status=="skipped")) AS total_exec, count(eval(status=="skipped")) AS skipped_exec by app savedsearch_name
| eval skip_ratio=round(((skipped_exec / total_exec) * 100),2)
| eval skip_ratio_nosplit=round(((skipped_exec_nosplit / total_exec_nosplit) * 100),2)

<008 - search 4> <share snapshot of the table>

| rest timeout=600 splunk_server=* /servicesNS/-/-/configs/conf-limits
| rename splunk_server AS instance
| stats max(base_max_searches) max(max_searches_per_cpu) by instance
| rename max(*) AS *

<009 - search 5> <share csv output>

| rest splunk_server=local /servicesNS/-/-/saved/searches 
| fields search title author cron_schedule eai:acl.app eai:acl.sharing dispatch.earliest_time dispatch.latest_time action.summary_index *real*time* 
| rename eai:* as * 
| stats values(*) as * by title

<010 - search 6 - make sure that you run this with host field containing search heads only> <share snapshot of the table>

index=_introspection host=sh* sourcetype=splunk_resource_usage component=PerProcess data.search_props.sid::* earliest=-w@w+1d latest=-w@w+2d 
| bin _time span=10s 
| stats dc(data.search_props.sid) AS distinct_search_count by _time, host 
| stats sum(distinct_search_count) AS distinct_search_count by _time 
| stats avg(distinct_search_count) AS "average_search_concurrency" p90(distinct_search_count) AS "p90_search_concurrency", p99(distinct_search_count) AS "p99_search_concurrency" max(distinct_search_count) AS "max_search_concurrency" 
| foreach p* av* max*
[| eval <<FIELD>>=round(<<FIELD>>,1) ]
| addcoltotals

<011 - search schedule> <share the csv with us>

index=_internal source=*scheduler.log sourcetype=scheduler SavedSplunker status!=delegated_* earliest=-7d@d latest=@d 
| eval lag_time = dispatch_time - scheduled_time 
| stats count as total_count count(eval(status="success")) as success_count count(eval(status="skipped")) as skipped_count count(eval(status="continued")) AS continued_count max(run_time) p90(run_time) p50(run_time) min(run_time) max(lag_time) p90(lag_time) p50(lag_time) min(lag_time) earliest(lag_time) latest(lag_time) by host app savedsearch_name user 
| join type=left app savedsearch_name 
    [ rest splunk_server=local "/servicesNS/-/-/saved/searches/" search="is_scheduled=1" search="disabled=0" 
    | fields title, eai:acl.app, eai:acl.owner, cron_schedule, realtime_schedule durable.* dispatch.earliest_time, dispatch.latest_time, schedule_window, allow_skew, actions, schedule_priority, dispatch.ttl, auto_summarize action.summary_index
    | rename title as savedsearch_name eai:acl.* as * dispatch.* as dispatch_* durable.* as durable_* action.* as action_* cron_schedule as cron ] 
| rex field=cron "^(?<cron_m>\S+)\s+(?<cron_h>\S+)\s+(?<cron_date>\S+)\s+(?<cron_month>\S+)\s+(?<cron_dow>\S+)" 
| rex field=cron_dow max_match=0 "(?<cron_dow_at>\d+)" 
| rex field=cron_month max_match=0 "(?<cron_month_at>\d+)" 
| rex field=cron_date max_match=0 "(?<cron_date_at>\d+)" 
| rex field=cron_h max_match=0 "(?<cron_h_at>\d+)" 
| rex field=cron_h "\/(?<cron_h_every>.+)" 
| rex field=cron_h "(?<cron_h_thr>\d+-\d+)" 
| rex field=cron_m max_match=0 "(?<cron_m_at>\d+)" 
| rex field=cron_m "\/(?<cron_m_every>.+)" 
| eval cron_h_at = if(match(cron_h, "^\d+-"), mvindex(cron_h_at, 0), if(cron_h = "*", 0, cron_h_at)) 
| eval cron_h_at0 = mvindex(cron_h_at,0), cron_h_at1 = mvindex(cron_h_at,1) 
| eval cron_m_at = if(match(cron_m, "^\d+-"), mvindex(cron_m_at, 0), if(cron_m = "*", 0, cron_m_at)) 
| eval cron_m_at0 = mvindex(cron_m_at,0), cron_m_at1 = mvindex(cron_m_at,1) 
| eval interval = case(isnotnull(cron_dow_at), 86400*7, isnotnull(cron_month_at), 86400*365, isnotnull(cron_date_at), 86400*28, isnotnull(cron_h_every), cron_h_every * 3600, isnotnull(cron_h_thr), 3600, mvcount(cron_h_at)>1, (cron_h_at1 - cron_h_at0) * 3600, cron_m = "*", 60, isnotnull(cron_m_every), cron_m_every * 60, isnum(cron_m) AND cron_h = "*", 3600, mvcount(cron_m_at)>1, (cron_m_at1 - cron_m_at0) * 60, isnotnull(cron), 86400) 
| addinfo 
| eval period = info_max_time - info_min_time 
| eval interval = if(isnull(interval), ceil(period / total_count / 60) * 60, interval) 
| eval interval_fill = round(('max(run_time)' / interval) , 2) 
| eval cron_run = case(cron_m="*", "run_every_1", isnotnull(cron_m_every), "run_every_".cron_m_every, isnum(cron_m_at), "run_at_".(cron_m_at - 0), 1=1, cron_m) 
| eval cron_run = if(isnull(cron_run), if(interval >= 3600, "run_at_0", "run_every_".(interval/60)), cron_run) 
| fields host app savedsearch_name user cron cron_run interval max(run_time) interval_fill total_count success_count skipped_count continued_count max(lag_time) earliest(lag_time) latest(lag_time) dispatch_earliest_time dispatch_latest_time realtime_schedule schedule_priority schedule_window allow_skew workload_pool dispatch_ttl durable_* auto_summarize action_summary_index 
| sort 0 - interval_fill

